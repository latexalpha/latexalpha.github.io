<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>神经网络结构 - ZSY&#039;s Blog</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="ZSY&#039;s Blogs"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="ZSY&#039;s Blogs"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="本文主要介绍了一些神经网络结构的基础知识。"><meta property="og:type" content="blog"><meta property="og:title" content="神经网络结构"><meta property="og:url" content="https://latexalpha.github.io/4d887eb4400d/"><meta property="og:site_name" content="ZSY&#039;s Blog"><meta property="og:description" content="本文主要介绍了一些神经网络结构的基础知识。"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://ars.els-cdn.com/content/image/1-s2.0-S0045782520304114-gr1.jpg"><meta property="article:published_time" content="2022-02-11T12:39:23.000Z"><meta property="article:modified_time" content="2024-04-19T06:37:32.109Z"><meta property="article:author" content="Shangyu ZHAO"><meta property="article:tag" content="Deep Learning"><meta property="article:tag" content="Neural Network"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="https://ars.els-cdn.com/content/image/1-s2.0-S0045782520304114-gr1.jpg"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://latexalpha.github.io/4d887eb4400d/"},"headline":"神经网络结构","image":["https://ars.els-cdn.com/content/image/1-s2.0-S0045782520304114-gr1.jpg"],"datePublished":"2022-02-11T12:39:23.000Z","dateModified":"2024-04-19T06:37:32.109Z","author":{"@type":"Person","name":"Shangyu ZHAO"},"publisher":{"@type":"Organization","name":"ZSY's Blog","logo":{"@type":"ImageObject","url":"https://latexalpha.github.io/img/logo.png"}},"description":"本文主要介绍了一些神经网络结构的基础知识。"}</script><link rel="canonical" href="https://latexalpha.github.io/4d887eb4400d/"><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.7.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.png" alt="ZSY&#039;s Blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/archives">博客归档</a><a class="navbar-item" href="/academic-index">学术索引</a><a class="navbar-item" href="/academic-resources">学术资源</a><a class="navbar-item" href="/github-repos">代码仓库</a><a class="navbar-item" href="/official-projects">文档合集</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Google" href="https://www.google.com/"><i class="fab fa-google"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="目录" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-9-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2022-02-11T12:39:23.000Z" title="2022/2/11 20:39:23">2022-02-11</time>发表</span><span class="level-item"><time dateTime="2024-04-19T06:37:32.109Z" title="2024/4/19 14:37:32">2024-04-19</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E7%9F%A5%E8%AF%86%E5%AD%A6%E4%B9%A0/">知识学习</a></span><span class="level-item">18 分钟读完 (大约2730个字)</span></div></div><h1 class="title is-3 is-size-4-mobile">神经网络结构</h1><div class="content"><p>本文主要介绍了一些神经网络结构的基础知识。</p>
<span id="more"></span>


<ul>
<li><a target="_blank" rel="noopener" href="https://developer.ibm.com/articles/cc-machine-learning-deep-learning-architectures/">Deep learning architectures - IBM Developer</a></li>
<li><a target="_blank" rel="noopener" href="https://www.skynettoday.com/overviews/neural-net-history">A Brief History of Neural Nets and Deep Learning</a></li>
</ul>
<h2 id="前馈神经网络-Feed-Forward-Neural-Network-FNN-FFNN"><a href="#前馈神经网络-Feed-Forward-Neural-Network-FNN-FFNN" class="headerlink" title="前馈神经网络 Feed-Forward Neural Network (FNN&#x2F;FFNN)"></a>前馈神经网络 Feed-Forward Neural Network (FNN&#x2F;FFNN)</h2><p>前馈神经网络是最简单、最基本的神经网络结构。</p>
<p>这里需要区分一下前馈神经网络和多层感知机 (Multilayer Perceptron, MLP) 的区别，简单来说，多层感知机是一种具有三层结构的前馈神经网络。</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Feedforward_neural_network">Feedforward neural network - Wikipedia</a></li>
<li><a target="_blank" rel="noopener" href="https://kobiso.github.io/research/research-ffnn/">Feed-Forward Neural Network (FFNN)</a></li>
<li><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Multilayer_perceptron">Multilayer perceptron - Wikipedia</a></li>
<li><a target="_blank" rel="noopener" href="https://www.sciencedirect.com/topics/computer-science/multilayer-perceptron">Multilayer Perceptron - an overview | ScienceDirect Topics</a></li>
<li><a target="_blank" rel="noopener" href="https://datascience.stackexchange.com/questions/72589/what-is-the-difference-between-fc-and-mlp-in-as-used-in-pointnet">pytorch - What is the difference between FC and MLP in as used in PointNet? - Data Science Stack Exchange</a></li>
</ul>
<h2 id="卷积神经网络-Convolutional-Neural-Network-CNN"><a href="#卷积神经网络-Convolutional-Neural-Network-CNN" class="headerlink" title="卷积神经网络 Convolutional Neural Network (CNN)"></a>卷积神经网络 Convolutional Neural Network (CNN)</h2><h3 id="基础卷积神经网络"><a href="#基础卷积神经网络" class="headerlink" title="基础卷积神经网络"></a>基础卷积神经网络</h3><p>卷积神经网络（Convolutional Neural Network, CNN）是一种前馈神经网络，它的人工神经元可以响应一部分覆盖范围内的周围单元，对于大型图像处理有出色表现。</p>
<p>卷积神经网络由一个或多个卷积层和顶端的全连通层（对应经典的神经网络）组成，同时也包括关联权重和池化层（pooling layer）。这一结构使得卷积神经网络能够利用输入数据的二维结构。与其他深度学习结构相比，卷积神经网络在图像和语音识别方面能够给出更好的结果。这一模型也可以使用反向传播算法进行训练。相比较其他深度、前馈神经网络，卷积神经网络需要考量的参数更少，使之成为一种颇具吸引力的深度学习结构。</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C">卷积神经网络 - 维基百科，自由的百科全书 (wikipedia.org)</a></li>
<li><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Convolutional_neural_network">Convolutional neural network - Wikipedia</a></li>
</ul>
<h3 id="时间卷积网络-Temporal-Convolutional-Networks-TCN"><a href="#时间卷积网络-Temporal-Convolutional-Networks-TCN" class="headerlink" title="时间卷积网络 Temporal Convolutional Networks (TCN)"></a>时间卷积网络 Temporal Convolutional Networks (TCN)</h3><ul>
<li><a target="_blank" rel="noopener" href="https://paperswithcode.com/method/dilated-convolution">Dilated Convolution Explained | Papers With Code</a></li>
<li><a target="_blank" rel="noopener" href="https://towardsdatascience.com/temporal-convolutional-networks-the-next-revolution-for-time-series-8990af826567">Temporal Convolutional Networks, The Next Revolution for Time-Series?</a></li>
</ul>
<h2 id="循环神经网络-Recurrent-Neural-Network-RNN"><a href="#循环神经网络-Recurrent-Neural-Network-RNN" class="headerlink" title="循环神经网络 Recurrent Neural Network (RNN)"></a>循环神经网络 Recurrent Neural Network (RNN)</h2><h3 id="经典-RNN"><a href="#经典-RNN" class="headerlink" title="经典 RNN"></a>经典 RNN</h3><p><a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C">循环神经网络</a> <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Recurrent_neural_network">Recurrent neural network</a></p>
<p>循环神经网络（Recurrent neural network：RNN）是神经网络的一种。单纯的 RNN 因为无法处理随着递归，权重指数级爆炸或梯度消失问题，难以捕捉长期时间关联；而结合不同的 LSTM 可以很好解决这个问题。</p>
<p>时间循环神经网络可以描述动态时间行为，因为和前馈神经网络（feedforward neural network）接受较特定结构的输入不同，RNN 将状态在自身网络中循环传递，因此可以接受更广泛的时间序列结构输入。手写识别是最早成功利用 RNN 的研究结果。</p>
<p>A recurrent neural network (RNN) is a class of artificial neural networks where connections between nodes from a directed graph along a temporal sequence. This allows it to exhibit temporal dynamic behavior. Derived from feedforward neural networks, RNNs can use their internal state (memory) to process variable length sequences of inputs. This makes them applicable to tasks such as unsegmented, connected handwriting recognition or speech recognition.</p>
<p>The term “recurrent neural network” is used to indiscriminately to refer to two broad classes of network with a similar general structure, where one is finite impulse and the other is infinite impulse. Both classes of networks exhibit temporal dynamic behavior. A finite impulse recurrent network is a directed acyclic graph that can be unrolled and replaced with a strictly feedforward neural network, while an infinite impulse recurrent network is a directed cyclic graph that can not be unrolled.</p>
<p>Both finite impulse and infinite impulse recurrent networks can have additional stored states, and the storage can be under direct control by the neural network. The storage can also be replaced by another network or graph if that incorporates time delays or has feedback loops. Such controlled states are referred to as gated state or gated memory, and are part of [[long short-term memory]] networks (LSTMs) and gated recurrent units. This is also called Feedback Neural Network (FNN).</p>
<p>In typical libraries like PyTorch <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Just-in-time_compilation">Just-in-time compilation</a> plays an important role in efficiently implementing recurrent neural networks.</p>
<p><a target="_blank" rel="noopener" href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">The Unreasonable Effectiveness of Recurrent Neural Networks</a></p>
<h3 id="长短期记忆-Long-Short-Term-Memory-LSTM"><a href="#长短期记忆-Long-Short-Term-Memory-LSTM" class="headerlink" title="长短期记忆 Long Short-Term Memory (LSTM)"></a>长短期记忆 Long Short-Term Memory (LSTM)</h3><p><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Long_short-term_memory">Long Short-Term Memory</a></p>
<p>长短期记忆（英语：Long Short-Term Memory，LSTM）是一种时间循环神经网络（RNN），论文首次发表于 1997 年。由于独特的设计结构，LSTM 适合于处理和预测时间序列中间隔和延迟非常长的重要事件。</p>
<p>LSTM 的表现通常比时间循环神经网络及隐马尔科夫模型（HMM）更好，比如用在不分段连续手写识别上。2009 年，用 LSTM 构建的人工神经网络模型赢得过 ICDAR 手写识别比赛冠军。LSTM 还普遍用于自主语音识别，2013 年运用 TIMIT 自然演讲数据库达成 17.7%错误率的纪录。作为非线性模型，LSTM 可作为复杂的非线性单元用于构造更大型深度神经网络。</p>
<p><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0045782520304114-gr1.jpg" alt="Schematic of deep LSTM networks"></p>
<ul>
<li><a target="_blank" rel="noopener" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">Understanding LSTM Networks</a></li>
<li><a target="_blank" rel="noopener" href="https://towardsdatascience.com/3-steps-to-forecast-time-series-lstm-with-tensorflow-keras-ba88c6f05237">3 Steps to Forecast Time Series: LSTM with TensorFlow Keras</a></li>
</ul>
<h3 id="门控循环单元-Gated-Recurrent-Unit-GRU"><a href="#门控循环单元-Gated-Recurrent-Unit-GRU" class="headerlink" title="门控循环单元 Gated Recurrent Unit (GRU)"></a>门控循环单元 Gated Recurrent Unit (GRU)</h3><ul>
<li><p>Wikipedia</p>
<p><strong>Gated recurrent units</strong> are a gating mechanism in recurrent networks, introduced in 2014 by Kyunghyun Cho al. The GRU is like a a long short-term memory (LSTM) with a forget gate, but with fewer parameters than LSTM, as it lacks an output gate. GRU’s performance on certain tasks of polyphonic music modeling and natural language processing was found to be similar to that of LSTM. GRUs have been shown to exhabit better performance on certain smaller and less frequent datasets.</p>
</li>
<li><p>Architecture</p>
</li>
</ul>
<p>There are several variations on the full gated unit, with gating done using the previous hidden state and the bias in various combinations, and a simplified form called minimal gated unit.</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/dpengwang/article/details/91666374">GRU(Gated recurrent unit)</a></li>
<li><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s?__biz=Mzg5ODAzMTkyMg==&mid=2247484578&idx=1&sn=0dcf07d92a0338cbea462f9524f2421f#wechat_redirect">一步一步动画图解 LSTM 和 GRU，没有数学，包你看的明白！</a></li>
<li><a target="_blank" rel="noopener" href="https://www.cnblogs.com/bamtercelboo/p/7469005.html">基于 pytorch 的 CNN、LSTM 神经网络模型调参小结</a></li>
</ul>
<h2 id="残差网络-ResNet"><a href="#残差网络-ResNet" class="headerlink" title="残差网络 ResNet"></a>残差网络 ResNet</h2><p>The operator ⊙ denotes the Hadamard product in the following.</p>
<p>残差网络是为了解决深度神经网络（DNN）隐藏层过多时的网络退化问题而提出。退化（degradation）问题是指：当网络隐藏层变多时，网络的准确度达到饱和然后急剧退化，而且这个退化不是由于过拟合引起的。</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://sota.jiqizhixin.com/models/methods/ca45b507-67a0-4e95-a906-23cb374d9e4a">Residual Network(ResNet) | SOTA！模型 (jiqizhixin.com)</a></li>
<li>[[ResNet.pdf]]</li>
<li><a target="_blank" rel="noopener" href="https://www.analyticsvidhya.com/blog/2021/08/how-to-code-your-resnet-from-scratch-in-tensorflow/">How to code your ResNet from scratch in Tensorflow?  - Analytics Vidhya</a></li>
</ul>
<h2 id="编码器和变分自编码器-Autoencoders-and-Variational-Autoencoders-AE-VAE"><a href="#编码器和变分自编码器-Autoencoders-and-Variational-Autoencoders-AE-VAE" class="headerlink" title="编码器和变分自编码器 Autoencoders and Variational Autoencoders (AE &amp; VAE)"></a>编码器和变分自编码器 Autoencoders and Variational Autoencoders (AE &amp; VAE)</h2><ul>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/27549418">花式解释 AutoEncoder 与 VAE</a></li>
<li><a target="_blank" rel="noopener" href="https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73">Understanding Variational Autoencoders (VAEs)</a></li>
<li><a target="_blank" rel="noopener" href="https://www.jeremyjordan.me/autoencoders/">Introduction to autoencoders.</a></li>
<li><a target="_blank" rel="noopener" href="https://machinelearningmastery.com/develop-encoder-decoder-model-sequence-sequence-prediction-keras/">How to Develop an Encoder-Decoder Model for Sequence-to-Sequence Prediction in Keras - Machine Learning Mastery</a></li>
</ul>
<h2 id="生成式对抗网络-Generative-Adversarial-Networks-GAN"><a href="#生成式对抗网络-Generative-Adversarial-Networks-GAN" class="headerlink" title="生成式对抗网络 Generative Adversarial Networks (GAN)"></a>生成式对抗网络 Generative Adversarial Networks (GAN)</h2><p><a target="_blank" rel="noopener" href="https://towardsdatascience.com/understanding-generative-adversarial-networks-gans-cd6e4651a29?gi=6fbd1f78dcd3">Understanding Generative Adversarial Networks (GANs) | by Joseph Rocca | Towards Data Science</a></p>
<p>Generative Adversarial Networks (GANs for short) have had a huge success since they were introduced in 2014 by Ian J. Goodfellow and co-authors in the article <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1406.2661">Generative Adversarial Nets</a>.</p>
<h3 id="生成模型"><a href="#生成模型" class="headerlink" title="生成模型"></a>生成模型</h3><ul>
<li><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Generative_model#:~:text=Terminology%20is%20inconsistent%2C%20%5Ba%5D%20but%20three%20major%20types,model%20are%20also%20referred%20to%20loosely%20as%20%22discriminative%22.">Generative model - Wikipedia</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/432229453">Generative Models，VAE，ELBO</a></li>
</ul>
<h2 id="图神经网络-Graph-Neural-Network-GNN"><a href="#图神经网络-Graph-Neural-Network-GNN" class="headerlink" title="图神经网络 Graph Neural Network (GNN)"></a>图神经网络 Graph Neural Network (GNN)</h2><ul>
<li><a target="_blank" rel="noopener" href="https://www.giulianopertile.com/blog/the-definitive-guide-to-graph-problems/">The Definitive Guide to Graph Problems | Giuliano Pertile</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/152371207">一文看懂 25 个神经网络模型</a></li>
<li><a target="_blank" rel="noopener" href="https://medium.com/@saikatbhattacharya/an-introduction-to-graph-neural-network-part-i-5c2e09f29ec2">An introduction to Graph Neural Network - Part I</a></li>
</ul>
<h2 id="Attention-and-Transformer"><a href="#Attention-and-Transformer" class="headerlink" title="Attention and Transformer"></a>Attention and Transformer</h2><p><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Attention_(machine_learning)">Attention (machine learning) - Wikipedia</a></p>
<h3 id="Attention-and-Self-Attention"><a href="#Attention-and-Self-Attention" class="headerlink" title="Attention and Self-Attention"></a>Attention and Self-Attention</h3><p>之前的 seq2seq 模型难以处理长序列，于是 Attention 被提出。<br><a target="_blank" rel="noopener" href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention) – Jay Alammar – Visualizing machine learning one concept at a time. (jalammar.github.io)</a></p>
<p>Atttention 的目的在于使得模型可以专注于输入中的核心部分，但是 Attention 存在的一个问题在于计算量和输入长度的平方成正比。<br><a target="_blank" rel="noopener" href="https://towardsdatascience.com/demystifying-efficient-self-attention-b3de61b9b0fb">Demystifying efficient self-attention | by Thomas van Dongen | Towards Data Science</a></p>
<p>Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences.</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1706.03762">[1706.03762] Attention Is All You Need (arxiv.org)</a> [Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence.]</li>
<li><a target="_blank" rel="noopener" href="https://medium.com/@geetkal67/attention-networks-a-simple-way-to-understand-self-attention-f5fb363c736d">Attention Networks: A simple way to understand Self Attention | by Geetansh Kalra | Medium</a></li>
<li><a target="_blank" rel="noopener" href="https://medium.com/mlearning-ai/whats-the-difference-between-self-attention-and-attention-in-transformer-architecture-3780404382f3">Self attention vs attention in transformers | MLearning.ai (medium.com)</a></li>
<li><a target="_blank" rel="noopener" href="https://slds-lmu.github.io/seminar_nlp_ss20/attention-and-self-attention-for-nlp.html">Chapter 8 Attention and Self-Attention for NLP | Modern Approaches in Natural Language Processing (slds-lmu.github.io)</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/47063917">Attention 机制详解（一）——Seq2Seq 中的 Attention - 知乎 (zhihu.com)</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/47282410">Attention 机制详解（二）——Self-Attention 与 Transformer - 知乎 (zhihu.com)</a></li>
<li><a target="_blank" rel="noopener" href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Attention">tf.keras.layers.Attention  |  TensorFlow v2.13.0</a></li>
<li><a target="_blank" rel="noopener" href="https://towardsdatascience.com/demystifying-efficient-self-attention-b3de61b9b0fb">Demystifying efficient self-attention | by Thomas van Dongen | Towards Data Science</a> [文章里介绍了各种 Attention 的变体，尤其提到了降低运算复杂度的方法。]</li>
</ul>
<h3 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h3><ul>
<li><a target="_blank" rel="noopener" href="https://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time. (jalammar.github.io)</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/82312421">十分钟理解 Transformer - 知乎 (zhihu.com)</a></li>
<li><a target="_blank" rel="noopener" href="https://luweikxy.gitbook.io/machine-learning-notes/self-attention-and-transformer">Self-Attention 和 Transformer - machine-learning-notes (gitbook.io)</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/48508221">详解 Transformer （Attention Is All You Need） - 知乎 (zhihu.com)</a></li>
</ul>
<h3 id="Temporal-Fusion-Transformer-TFT"><a href="#Temporal-Fusion-Transformer-TFT" class="headerlink" title="Temporal Fusion Transformer (TFT)"></a>Temporal Fusion Transformer (TFT)</h3><ul>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/383036166">Temporal fusion transformer</a></li>
<li><a target="_blank" rel="noopener" href="https://towardsdatascience.com/temporal-fusion-transformer-time-series-forecasting-with-deep-learning-complete-tutorial-d32c1e51cd91">Temporal Fusion Transformer: Time Series Forecasting with Deep Learning - Complete Tutorial</a></li>
<li><a target="_blank" rel="noopener" href="https://jrodthoughts.medium.com/transformers-for-time-series-inside-googles-temporal-fusion-transformers-7e6331910a3e">Transformers for Time Series? Inside Google’s Temporal Fusion Transformers</a></li>
</ul>
<h3 id="FNet-Mixing-Tokens-with-Fourier-Transforms"><a href="#FNet-Mixing-Tokens-with-Fourier-Transforms" class="headerlink" title="FNet: Mixing Tokens with Fourier Transforms"></a>FNet: Mixing Tokens with Fourier Transforms</h3><ul>
<li><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2105.03824">[2105.03824] FNet: Mixing Tokens with Fourier Transforms (arxiv.org)</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://aistudio.baidu.com/projectdetail/3158413">论文复现：FNet——使用傅里叶变换替代自注意力层 - 飞桨 AI Studio (baidu.com)</a></p>
</li>
</ul>
<h2 id="Neural-Operator"><a href="#Neural-Operator" class="headerlink" title="Neural Operator"></a>Neural Operator</h2><ul>
<li><a target="_blank" rel="noopener" href="https://zongyi-li.github.io/neural-operator/">Neural Operator (zongyi-li.github.io)</a></li>
</ul>
<h3 id="Fourier-Neural-Operator-FNO"><a href="#Fourier-Neural-Operator-FNO" class="headerlink" title="Fourier Neural Operator (FNO)"></a>Fourier Neural Operator (FNO)</h3><ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2010.08895">[2010.08895] Fourier Neural Operator for Parametric Partial Differential Equations (arxiv.org)</a></li>
<li><a target="_blank" rel="noopener" href="https://zongyi-li.github.io/blog/2020/fourier-pde/">Zongyi Li | Fourier Neural Operator (zongyi-li.github.io)</a></li>
<li><a target="_blank" rel="noopener" href="https://zongyi-li.github.io/blog/2020/graph-pde/">Zongyi Li | Graph Neural Operator for PDEs (zongyi-li.github.io)</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/neuraloperator/neuraloperator">GitHub - neuraloperator&#x2F;neuraloperator: Learning in infinite dimension with neural operators.</a></li>
</ul>
<h3 id="Graph-Neural-Operator-GNO"><a href="#Graph-Neural-Operator-GNO" class="headerlink" title="Graph Neural Operator (GNO)"></a>Graph Neural Operator (GNO)</h3><ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2003.03485">[2003.03485] Neural Operator: Graph Kernel Network for Partial Differential Equations (arxiv.org)</a></li>
</ul>
</div><div class="article-licensing box"><div class="licensing-title"><p>神经网络结构</p><p><a href="https://latexalpha.github.io/4d887eb4400d/">https://latexalpha.github.io/4d887eb4400d/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>作者</h6><p>Shangyu ZHAO</p></div></div><div class="level-item is-narrow"><div><h6>发布于</h6><p>2022-02-11</p></div></div><div class="level-item is-narrow"><div><h6>更新于</h6><p>2024-04-19</p></div></div><div class="level-item is-narrow"><div><h6>许可协议</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/Deep-Learning/">Deep Learning</a><a class="link-muted mr-2" rel="tag" href="/tags/Neural-Network/">Neural Network</a></div><!--!--></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">喜欢这篇文章？打赏一下作者吧</h3><div class="buttons is-centered"><a class="button donate" data-type="alipay"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>支付宝</span><span class="qrcode"><img src="https://raw.githubusercontent.com/latexalpha/image_bed/main/images24/202401181822005.jpg" alt="支付宝"></span></a><a class="button donate" data-type="wechat"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>微信</span><span class="qrcode"><img src="https://raw.githubusercontent.com/latexalpha/image_bed/main/images24/202401181822198.jpg" alt="微信"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/09479062e9b3/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">编程经验积累</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/b13f8cf9eb56/"><span class="level-item">深度学习基础知识</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><!--!--></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1 is-sticky"><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/%E7%9F%A5%E8%AF%86%E5%AD%A6%E4%B9%A0/"><span class="level-start"><span class="level-item">知识学习</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%BD%AF%E4%BB%B6%E4%BD%BF%E7%94%A8/"><span class="level-start"><span class="level-item">软件使用</span></span><span class="level-end"><span class="level-item tag">13</span></span></a></li><li><a class="level is-mobile" href="/categories/%E9%A1%B9%E7%9B%AE%E5%BC%80%E5%8F%91/"><span class="level-start"><span class="level-item">项目开发</span></span><span class="level-end"><span class="level-item tag">12</span></span></a></li></ul></div></div></div><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">目录</h3><ul class="menu-list"><li><a class="level is-mobile" href="#前馈神经网络-Feed-Forward-Neural-Network-FNN-FFNN"><span class="level-left"><span class="level-item">前馈神经网络 Feed-Forward Neural Network (FNN/FFNN)</span></span></a></li><li><a class="level is-mobile" href="#卷积神经网络-Convolutional-Neural-Network-CNN"><span class="level-left"><span class="level-item">卷积神经网络 Convolutional Neural Network (CNN)</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#基础卷积神经网络"><span class="level-left"><span class="level-item">基础卷积神经网络</span></span></a></li><li><a class="level is-mobile" href="#时间卷积网络-Temporal-Convolutional-Networks-TCN"><span class="level-left"><span class="level-item">时间卷积网络 Temporal Convolutional Networks (TCN)</span></span></a></li></ul></li><li><a class="level is-mobile" href="#循环神经网络-Recurrent-Neural-Network-RNN"><span class="level-left"><span class="level-item">循环神经网络 Recurrent Neural Network (RNN)</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#经典-RNN"><span class="level-left"><span class="level-item">经典 RNN</span></span></a></li><li><a class="level is-mobile" href="#长短期记忆-Long-Short-Term-Memory-LSTM"><span class="level-left"><span class="level-item">长短期记忆 Long Short-Term Memory (LSTM)</span></span></a></li><li><a class="level is-mobile" href="#门控循环单元-Gated-Recurrent-Unit-GRU"><span class="level-left"><span class="level-item">门控循环单元 Gated Recurrent Unit (GRU)</span></span></a></li></ul></li><li><a class="level is-mobile" href="#残差网络-ResNet"><span class="level-left"><span class="level-item">残差网络 ResNet</span></span></a></li><li><a class="level is-mobile" href="#编码器和变分自编码器-Autoencoders-and-Variational-Autoencoders-AE-VAE"><span class="level-left"><span class="level-item">编码器和变分自编码器 Autoencoders and Variational Autoencoders (AE &amp; VAE)</span></span></a></li><li><a class="level is-mobile" href="#生成式对抗网络-Generative-Adversarial-Networks-GAN"><span class="level-left"><span class="level-item">生成式对抗网络 Generative Adversarial Networks (GAN)</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#生成模型"><span class="level-left"><span class="level-item">生成模型</span></span></a></li></ul></li><li><a class="level is-mobile" href="#图神经网络-Graph-Neural-Network-GNN"><span class="level-left"><span class="level-item">图神经网络 Graph Neural Network (GNN)</span></span></a></li><li><a class="level is-mobile" href="#Attention-and-Transformer"><span class="level-left"><span class="level-item">Attention and Transformer</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Attention-and-Self-Attention"><span class="level-left"><span class="level-item">Attention and Self-Attention</span></span></a></li><li><a class="level is-mobile" href="#Transformer"><span class="level-left"><span class="level-item">Transformer</span></span></a></li><li><a class="level is-mobile" href="#Temporal-Fusion-Transformer-TFT"><span class="level-left"><span class="level-item">Temporal Fusion Transformer (TFT)</span></span></a></li><li><a class="level is-mobile" href="#FNet-Mixing-Tokens-with-Fourier-Transforms"><span class="level-left"><span class="level-item">FNet: Mixing Tokens with Fourier Transforms</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Neural-Operator"><span class="level-left"><span class="level-item">Neural Operator</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Fourier-Neural-Operator-FNO"><span class="level-left"><span class="level-item">Fourier Neural Operator (FNO)</span></span></a></li><li><a class="level is-mobile" href="#Graph-Neural-Operator-GNO"><span class="level-left"><span class="level-item">Graph Neural Operator (GNO)</span></span></a></li></ul></li></ul></div></div><script src="/js/toc.js" defer></script></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.png" alt="ZSY&#039;s Blog" height="28"><p class="is-size-7"><span>&copy; 2025 Shangyu ZHAO</span></p></a><p class="is-size-7"></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Hexo documentation" href="https://hexo.io/docs/"><i class="fas fa-book"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Hexo theme icarus" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-cn");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-right",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/javascript" id="MathJax-script" async>MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      },
      svg: {
        fontCache: 'global'
      },
      chtml: {
        matchFontHeight: false
      }
    };</script><script src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js"></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>